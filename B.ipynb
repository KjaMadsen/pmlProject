{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import pyro\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from torch.distributions import constraints\n",
    "import arviz as az\n",
    "\n",
    "import pyro.contrib.gp as gp\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import NUTS\n",
    "from pyro.infer import MCMC\n",
    "\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "smoke_test = \"CI\" in os.environ\n",
    "\n",
    "pyro.set_rng_seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(\n",
    "    plot_observed_data=False,\n",
    "    plot_predictions=False,\n",
    "    n_prior_samples=0,\n",
    "    model=None,\n",
    "    kernel=None,\n",
    "    n_test=500,\n",
    "    ax=None,\n",
    "    big_plot=False\n",
    "):\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    if plot_observed_data:\n",
    "        ax.plot(X.numpy(), f(X).numpy(), \"kx\")\n",
    "    if plot_predictions:\n",
    "        Xtest = torch.linspace(-1, 1, n_test)  # test inputs\n",
    "        # compute predictive mean and variance\n",
    "        with torch.no_grad():\n",
    "            if type(model) == gp.models.VariationalSparseGP:\n",
    "                mean, cov = model(Xtest, full_cov=True)\n",
    "            else:\n",
    "                mean, cov = model(Xtest, full_cov=True, noiseless=False)\n",
    "        sd = cov.diag().sqrt()  # standard deviation at each input point x\n",
    "        ax.plot(Xtest.numpy(), mean.numpy(), \"r\", lw=2)  # plot the mean\n",
    "        ax.fill_between(\n",
    "            Xtest.numpy(),  # plot the two-sigma uncertainty about the mean\n",
    "            (mean - 2.0 * sd).numpy(),\n",
    "            (mean + 2.0 * sd).numpy(),\n",
    "            color=\"C0\",\n",
    "            alpha=0.3,\n",
    "        )\n",
    "    if n_prior_samples > 0:  # plot samples from the GP prior\n",
    "        Xtest = torch.linspace(-1, 1, n_test)  # test inputs\n",
    "        noise = (\n",
    "            model.noise\n",
    "            if type(model) != gp.models.VariationalSparseGP\n",
    "            else model.likelihood.variance\n",
    "        )\n",
    "        cov = kernel.forward(Xtest) + noise.expand(n_test).diag()\n",
    "        samples = dist.MultivariateNormal(\n",
    "            torch.zeros(n_test), covariance_matrix=cov\n",
    "        ).sample(sample_shape=(n_prior_samples,))\n",
    "        ax.plot(Xtest.numpy(), samples.numpy().T, lw=2, alpha=0.4)\n",
    "    if big_plot == True:\n",
    "        x_seq = np.arange(start=-1,stop=1,step=0.005)\n",
    "        #plt.plot(x_seq,f(x_seq))\n",
    "        #plt.xlabel(\"x\")\n",
    "        #plt.ylabel(\"f(x)\")\n",
    "        legend_elements = [Line2D([0], [0], color='r', lw=4, label='Predicted f(x)'),\n",
    "                   Line2D([0], [0], marker='s', color='w', label='Confidence area',\n",
    "                            markerfacecolor='cornflowerblue', markersize=15),\n",
    "                   \n",
    "                   Line2D([0], [0], marker='x', color='w', label='Observed points',\n",
    "                            markeredgecolor='0')]\n",
    "        ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "    ax.set_xlim(-1, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.sin(20*x)+2*np.cos(14*x)-2*np.sin(6*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "X = torch.tensor([-1,-0.5,0,0.5,1])\n",
    "y = f(X)\n",
    "# Let's plot the observed data\n",
    "plot(plot_observed_data=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "kernel = gp.kernels.RBF(input_dim=3)\n",
    "kernel.lengthscale = pyro.nn.PyroSample(dist.LogNormal(torch.tensor(-1.0), torch.tensor(1.0)))\n",
    "kernel.variance = pyro.nn.PyroSample(dist.LogNormal(torch.tensor(0.), torch.tensor(2.0)))\n",
    "gpr = gp.models.GPRegression(X, y, kernel, noise=torch.tensor(0.0001)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NUTS Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMC\n",
    "hmc_kernel = NUTS(gpr.model)\n",
    "mcmc = MCMC(hmc_kernel, num_samples=100, num_chains=5, warmup_steps=100)\n",
    "mcmc.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples = mcmc.get_samples(num_samples=500)\n",
    "lengthscales = posterior_samples['kernel.lengthscale']\n",
    "variances = posterior_samples['kernel.variance']\n",
    "\n",
    "kernel = gp.kernels.RBF(input_dim=3, lengthscale=lengthscales.mean(), variance=variances.mean())\n",
    "gpr= gp.models.GPRegression(X, y, kernel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(torch.log(posterior_samples['kernel.lengthscale']),torch.log(posterior_samples['kernel.variance']))\n",
    "plt.title('Sample from posterior, N = 500')\n",
    "plt.xlabel('log Lengthscale')\n",
    "plt.ylabel('log Variance')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = az.from_pyro(mcmc)\n",
    "summary = az.summary(data)\n",
    "print(summary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Big plot\n",
    "\n",
    "Add confidence area and $f(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot(model=gpr,ax=ax, plot_observed_data=True, plot_predictions=True, big_plot=True)\n",
    "ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\"); ax.set_title(\"Results of funciton fitting\")\n",
    "fig.savefig(\"results_B/funcfit.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plotb(x,y,\n",
    "    plot_observed_data=False,\n",
    "    plot_predictions=False,\n",
    "    n_prior_samples=0,\n",
    "    model=None,\n",
    "    kernel=None,\n",
    "    n_test=500,\n",
    "    ax=None,\n",
    "    big_plot=False\n",
    "):\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    if plot_observed_data:\n",
    "        ax.plot(x.numpy(), y.numpy(), \"kx\")\n",
    "    if plot_predictions:\n",
    "        Xtest = torch.linspace(-1, 1, n_test)  # test inputs\n",
    "        # compute predictive mean and variance\n",
    "        with torch.no_grad():\n",
    "            if type(model) == gp.models.VariationalSparseGP:\n",
    "                mean, cov = model(Xtest, full_cov=True)\n",
    "            else:\n",
    "                mean, cov = model(Xtest, full_cov=True, noiseless=False)\n",
    "        sd = cov.diag().sqrt()  # standard deviation at each input point x\n",
    "        ax.plot(Xtest.numpy(), mean.numpy(), \"r\", lw=2)  # plot the mean\n",
    "        ax.fill_between(\n",
    "            Xtest.numpy(),  # plot the two-sigma uncertainty about the mean\n",
    "            (mean - 2.0 * sd).numpy(),\n",
    "            (mean + 2.0 * sd).numpy(),\n",
    "            color=\"C0\",\n",
    "            alpha=0.3,\n",
    "        )\n",
    "    if n_prior_samples > 0:  # plot samples from the GP prior\n",
    "        Xtest = torch.linspace(-1, 1, n_test)  # test inputs\n",
    "        noise = (\n",
    "            model.noise\n",
    "            if type(model) != gp.models.VariationalSparseGP\n",
    "            else model.likelihood.variance\n",
    "        )\n",
    "        cov = kernel.forward(Xtest) + noise.expand(n_test).diag()\n",
    "        samples = dist.MultivariateNormal(\n",
    "            torch.zeros(n_test), covariance_matrix=cov\n",
    "        ).sample(sample_shape=(n_prior_samples,))\n",
    "        ax.plot(Xtest.numpy(), samples.numpy().T, lw=2, alpha=0.4)\n",
    "    if big_plot == True:\n",
    "        x_seq = np.arange(start=-1,stop=1,step=0.005)\n",
    "        ax.plot(Xtest, f(Xtest), color = 'green', label= \"True f(x)\")\n",
    "        \n",
    "        legend_elements = [Line2D([0], [0], color='r', lw=4, label='Predicted f(x)'),\n",
    "                   Line2D([0], [0], marker='s', color='w', label='Confidence area',\n",
    "                            markerfacecolor='cornflowerblue', markersize=15),\n",
    "                   Line2D([0], [0], marker='x', color='w', label='Observed points',\n",
    "                            markeredgecolor='0'),\n",
    "                    Line2D([0],[0], color='green', label=\"True f(x)\")]\n",
    "        ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "    ax.set_xlim(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "T = 10\n",
    "x = torch.clone(X).detach()\n",
    "x_star = torch.linspace(-1,1,200)\n",
    "y = f(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "store = []\n",
    "\n",
    "for k in range(1,T+1):\n",
    "    pyro.clear_param_store()\n",
    "    kernel = gp.kernels.RBF(input_dim=3)\n",
    "    kernel.lengthscale = pyro.nn.PyroSample(dist.LogNormal(torch.tensor(-1.0), torch.tensor(1.0)))\n",
    "    kernel.variance = pyro.nn.PyroSample(dist.LogNormal(torch.tensor(0.), torch.tensor(2.0)))\n",
    "    gpr = gp.models.GPRegression(x, y, kernel, noise=torch.tensor(0.0001)) \n",
    "    # MCMC\n",
    "    hmc_kernel = NUTS(gpr.model)\n",
    "    mcmc = MCMC(hmc_kernel, num_samples=100, num_chains=5, warmup_steps=100)\n",
    "    mcmc.run()\n",
    "    posterior_samples = mcmc.get_samples(num_samples=500)\n",
    "    lengthscales = posterior_samples['kernel.lengthscale']\n",
    "    variances = posterior_samples['kernel.variance']\n",
    "\n",
    "    kernel = gp.kernels.RBF(input_dim=3, lengthscale=lengthscales.mean(), variance=variances.mean())\n",
    "    gpr = gp.models.GPRegression(x, y, kernel, noise=torch.tensor(0.0001))\n",
    "    with torch.no_grad():\n",
    "        m, cov = gpr(x_star, full_cov=True, noiseless=False)\n",
    "        v = cov.diag().sqrt()\n",
    "        d = torch.distributions.LogNormal(m, v).sample() #???\n",
    "        p = torch.argmin(d).unsqueeze(0) #???\n",
    "        x = torch.cat((x,x_star[p]))\n",
    "        y = torch.cat((y, d[p])) #???\n",
    "    \n",
    "    if k == 1 or k == 5 or k == 10:\n",
    "        store.append(copy(gpr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,1)\n",
    "fig.set_size_inches(13, 13)\n",
    "for i in range(3):\n",
    "    plotb(x[:5+i*5], y[:5+i*5],model=store[i], plot_predictions=True, big_plot=True, plot_observed_data=True,ax=ax[i])\n",
    "    ax[i].set_xlabel('x'); ax[i].set_ylabel('f(x)/predicted f(x)'); ax[i].set_title(f\"Algorithm with k={i*5}\")\n",
    "\n",
    "fig.savefig(\"results_B/optim.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('pmlEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55285a41da61f3b5ce1faec5fd530327c13b78ca6def06f28f058b818cfcab31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
